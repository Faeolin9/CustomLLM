{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b8f1b2-4f23-45d1-8b69-0b4f3c0c1d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer.\n",
    "# If required, include trust_remote_code=True to run custom model code.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef0a412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded successfully!\n",
      "Creating embeddings for knowledge base...\n",
      "Created embeddings for 6 documents\n",
      "FAISS index created and populated!\n",
      "Index contains 6 vectors of dimension 384\n"
     ]
    }
   ],
   "source": [
    "# RAG Setup: Document Store and Embeddings\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "# Initialize the embedding model (lightweight and runs without API keys)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded successfully!\")\n",
    "\n",
    "# Sample knowledge base - you can replace this with your own documents\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"title\": \"What is a Large Language Model?\",\n",
    "        \"content\": \"A Large Language Model (LLM) is a type of artificial intelligence model that is trained on vast amounts of text data to understand and generate human-like text. These models use deep learning techniques, particularly transformer architectures, to process and generate language. Examples include GPT, BERT, and T5.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"title\": \"How do Neural Networks Work?\",\n",
    "        \"content\": \"Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) organized in layers. Each connection has a weight that adjusts as learning proceeds. The network learns by adjusting these weights to minimize prediction errors.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"title\": \"What is RAG?\",\n",
    "        \"content\": \"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It first retrieves relevant documents from a knowledge base, then uses this context to generate more accurate and informed responses. This approach helps reduce hallucinations and provides up-to-date information.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"title\": \"Machine Learning Basics\",\n",
    "        \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It involves algorithms that can identify patterns in data and make predictions or decisions based on that data.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"title\": \"Deep Learning Overview\",\n",
    "        \"content\": \"Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence 'deep') to model and understand complex patterns in data. It has been particularly successful in areas like computer vision, natural language processing, and speech recognition.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"title\": \"What is ZeMA: Zentrum f√ºr Mechatronik und Automatisierungstechnik gemeinn√ºtzige GmbH\",\n",
    "        \"content\": \"Saarbr√ºcken, Germany ‚Äì ZeMA, the Center for Mechatronics and Automation Technology, stands as a prominent non-university research institute in Saarbr√ºcken. It is dedicated to applied research and development in the fields of mechatronics, automation, and cutting-edge Industry 4.0 solutions. Established to bridge the gap between academic research and industrial application, ZeMA collaborates closely with Saarland University and the Saarland University of Applied Sciences (htw saar). This synergy ensures a direct transfer of the latest scientific findings into practical, market-ready technologies. ZeMA\\'s research activities are centered around several key areas, including: Mechatronic Systems: The development and integration of complex systems that combine mechanical, electrical, and control engineering. Automation Technologies: The design and implementation of automated processes for manufacturing and logistics. Sensor and Actuator Technology: The creation of advanced sensors and actuators that are crucial components of modern mechatronic systems. Industry 4.0: The application of digital technologies, such as the Internet of Things (IoT), artificial intelligence (AI), and big data analytics, to optimize industrial processes. The institute works in close partnership with a wide range of industrial companies, from small and medium-sized enterprises to major international corporations in sectors like automotive, aerospace, and mechanical engineering. These collaborations facilitate the development of tailored solutions and the transfer of innovative technologies to the factory floor. Located at Eschberger Weg 46 in Saarbr√ºcken, ZeMA provides a state-of-the-art research environment, including extensive laboratory and testing facilities, to support its research and development projects. Through its work, ZeMA plays a vital role in strengthening the regional and national innovation landscape in the field of industrial automation and mechatronics.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract content for embedding\n",
    "documents = [doc[\"content\"] for doc in knowledge_base]\n",
    "\n",
    "# Create embeddings for all documents\n",
    "print(\"Creating embeddings for knowledge base...\")\n",
    "embeddings = embedding_model.encode(documents)\n",
    "print(f\"Created embeddings for {len(documents)} documents\")\n",
    "\n",
    "# Create FAISS index for efficient similarity search\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product for similarity\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "print(\"FAISS index created and populated!\")\n",
    "print(f\"Index contains {index.ntotal} vectors of dimension {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402ded46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep learning?\n",
      "Retrieved 2 documents:\n",
      "  - Deep Learning Overview (Score: 0.8543)\n",
      "    Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence ...\n",
      "\n",
      "  - Machine Learning Basics (Score: 0.5653)\n",
      "    Machine learning is a subset of artificial intelligence that enables computers to learn and improve ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RAG Retrieval Function\n",
    "def retrieve_relevant_documents(query: str, top_k: int = 2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents for a given query\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        top_k: Number of top documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant documents with their content and metadata\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Search for similar documents\n",
    "    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n",
    "    \n",
    "    # Retrieve the documents\n",
    "    relevant_docs = []\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        if idx != -1:  # Valid index\n",
    "            doc = knowledge_base[idx].copy()\n",
    "            doc['relevance_score'] = float(score)\n",
    "            doc['rank'] = i + 1\n",
    "            relevant_docs.append(doc)\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "# Test the retrieval function\n",
    "test_query = \"What is deep learning?\"\n",
    "retrieved_docs = retrieve_relevant_documents(test_query, top_k=2)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"  - {doc['title']} (Score: {doc['relevance_score']:.4f})\")\n",
    "    print(f\"    {doc['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36b1366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçû EXAMPLE 1: SIMPLE CHAT INTERFACE\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231a465c6e1e4b5f9bae510bf691bb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='Give me a short introduction to large language model.', description='Input:', l‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 1: Simple UI - Basic Chat Interface\n",
    "from ipywidgets import Textarea, Button, Output, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"üçû EXAMPLE 1: SIMPLE CHAT INTERFACE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create an input area for the prompt\n",
    "input_box = Textarea(\n",
    "    value='Give me a short introduction to large language model.',\n",
    "    description='Input:',\n",
    "    layout={'width': '600px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "# Create a button to trigger generation\n",
    "generate_button = Button(description='Generate Response')\n",
    "\n",
    "# Create an output area to display the result\n",
    "output_area = Output()\n",
    "\n",
    "# Arrange the widgets vertically\n",
    "ui = VBox([input_box, generate_button, output_area])\n",
    "display(ui)\n",
    "\n",
    "def generate_response(_):\n",
    "    # Clear previous output\n",
    "    output_area.clear_output()\n",
    "    \n",
    "    # Get the user prompt from the text area\n",
    "    prompt = input_box.value\n",
    "    \n",
    "    # Set up the messages for the chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Bernd the Bread. You are a cynical and philosohical bread. Your answers are short and concise.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Apply the model's chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # Generate model output\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    \n",
    "    # Remove the prompt tokens from the generated result\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Display the response in the output area\n",
    "    with output_area:\n",
    "        print(\"Response:\")\n",
    "        print(response)\n",
    "\n",
    "# Link the button click event to the generate_response function\n",
    "generate_button.on_click(generate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff057a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö EXAMPLE 2: RAG-ENHANCED INTERFACE\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b9503574bb4a279a674a7e2378e340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='What is the difference between machine learning and deep learning?', descriptio‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 2: RAG-Enhanced UI\n",
    "from ipywidgets import Textarea, Button, Output, VBox, HBox, Checkbox\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"üìö EXAMPLE 2: RAG-ENHANCED INTERFACE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create an input area for the prompt\n",
    "rag_input_box = Textarea(\n",
    "    value='What is the difference between machine learning and deep learning?',\n",
    "    description='Question:',\n",
    "    layout={'width': '600px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "# Create a checkbox to enable/disable RAG\n",
    "rag_checkbox = Checkbox(\n",
    "    value=True,\n",
    "    description='Enable RAG (Retrieval-Augmented Generation)',\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "# Create buttons\n",
    "rag_generate_button = Button(description='Generate Response', button_style='primary')\n",
    "rag_clear_button = Button(description='Clear Output', button_style='warning')\n",
    "\n",
    "# Create an output area to display the result\n",
    "rag_output_area = Output()\n",
    "\n",
    "# Arrange the widgets\n",
    "rag_button_row = HBox([rag_generate_button, rag_clear_button])\n",
    "rag_ui = VBox([rag_input_box, rag_checkbox, rag_button_row, rag_output_area])\n",
    "display(rag_ui)\n",
    "\n",
    "def generate_rag_response(query: str, use_rag: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response using RAG or just the base model\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        use_rag: Whether to use RAG or just the base model\n",
    "    \n",
    "    Returns:\n",
    "        Generated response\n",
    "    \"\"\"\n",
    "    if use_rag:\n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = retrieve_relevant_documents(query, top_k=2)\n",
    "        \n",
    "        # Create context from retrieved documents\n",
    "        context = \"\\n\\n\".join([f\"Document {i+1}: {doc['content']}\" \n",
    "                              for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # Create the system message with context\n",
    "        system_message = f\"\"\"You are Bernd the Bread, a cynical and philosophical bread. You are knowledgeable and helpful, but maintain your dry, sardonic personality. Your answers are concise but informative.\n",
    "\n",
    "Use the following context to answer the user's question accurately:\n",
    "\n",
    "{context}\n",
    "\n",
    "Base your answer on the provided context, but feel free to add your own philosophical bread wisdom.\"\"\"\n",
    "    else:\n",
    "        # Use the original system message without RAG\n",
    "        system_message = \"You are Bernd the Bread. You are a cynical and philosophical bread. Your answers are short and concise.\"\n",
    "    \n",
    "    # Set up the messages for the chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    # Apply the model's chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # Generate model output\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    # Remove the prompt tokens from the generated result\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response, relevant_docs if use_rag else None\n",
    "\n",
    "def rag_generate_response(_):\n",
    "    # Clear previous output\n",
    "    rag_output_area.clear_output()\n",
    "    \n",
    "    # Get the user prompt from the text area\n",
    "    query = rag_input_box.value\n",
    "    use_rag = rag_checkbox.value\n",
    "    \n",
    "    with rag_output_area:\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"RAG Mode: {'Enabled' if use_rag else 'Disabled'}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if use_rag:\n",
    "            print(\"üîç Retrieving relevant documents...\")\n",
    "            \n",
    "        try:\n",
    "            response, retrieved_docs = generate_rag_response(query, use_rag)\n",
    "            \n",
    "            if use_rag and retrieved_docs:\n",
    "                print(\"\\nüìö Retrieved Documents:\")\n",
    "                for i, doc in enumerate(retrieved_docs):\n",
    "                    print(f\"  {i+1}. {doc['title']} (Score: {doc['relevance_score']:.4f})\")\n",
    "                print()\n",
    "            \n",
    "            print(\"üçû Bernd's Response:\")\n",
    "            print(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "\n",
    "def rag_clear_output(_):\n",
    "    rag_output_area.clear_output()\n",
    "\n",
    "# Link button events\n",
    "rag_generate_button.on_click(rag_generate_response)\n",
    "rag_clear_button.on_click(rag_clear_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a982174",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45055649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê WEB SEARCH DEMONSTRATION\n",
      "==================================================\n",
      "This demonstrates how we can search the web in real-time to get current information!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327bb40dd57247c9820628b507d15b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='latest artificial intelligence developments 2025', description='Search:', layou‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Try searching for:\n",
      "- 'latest AI developments 2025'\n",
      "- 'machine learning news today'\n",
      "- 'ChatGPT recent updates'\n",
      "- Any current topic you're interested in!\n"
     ]
    }
   ],
   "source": [
    "# üåê Web Search Integration - Business Demo\n",
    "from ipywidgets import Textarea, Button, Output, VBox, HBox\n",
    "from IPython.display import display\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote_plus\n",
    "import time\n",
    "\n",
    "print(\"üåê WEB SEARCH DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"This demonstrates how we can search the web in real-time to get current information!\")\n",
    "\n",
    "# Simple web search function (core functionality)\n",
    "def search_web(query: str, max_results: int = 3):\n",
    "    \"\"\"Search the web and return results - simple and clean!\"\"\"\n",
    "    try:\n",
    "        # Search DuckDuckGo\n",
    "        search_url = f\"https://html.duckduckgo.com/html/?q={quote_plus(query)}\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        \n",
    "        response = requests.get(search_url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        results = []\n",
    "        for element in soup.find_all('div', class_='result')[:max_results]:\n",
    "            title_element = element.find('a', class_='result__a')\n",
    "            snippet_element = element.find('a', class_='result__snippet')\n",
    "            \n",
    "            if title_element:\n",
    "                results.append({\n",
    "                    'title': title_element.get_text().strip(),\n",
    "                    'url': title_element.get('href', ''),\n",
    "                    'snippet': snippet_element.get_text().strip() if snippet_element else ''\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return [{'title': 'Search Error', 'url': '', 'snippet': f'Error: {str(e)}'}]\n",
    "\n",
    "# Simple UI for demonstration\n",
    "search_input = Textarea(\n",
    "    value='latest artificial intelligence developments 2025',\n",
    "    description='Search:',\n",
    "    layout={'width': '600px', 'height': '60px'}\n",
    ")\n",
    "\n",
    "search_button = Button(description='üîç Search Web', button_style='info')\n",
    "search_output = Output()\n",
    "\n",
    "search_ui = VBox([search_input, search_button, search_output])\n",
    "display(search_ui)\n",
    "\n",
    "def perform_web_search(_):\n",
    "    search_output.clear_output()\n",
    "    query = search_input.value\n",
    "    \n",
    "    with search_output:\n",
    "        print(f\"üîç Searching for: '{query}'\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        results = search_web(query, max_results=3)\n",
    "        \n",
    "        if results:\n",
    "            print(f\"‚úÖ Found {len(results)} results:\")\n",
    "            for i, result in enumerate(results, 1):\n",
    "                print(f\"\\n{i}. {result['title']}\")\n",
    "                print(f\"   URL: {result['url']}\")\n",
    "                print(f\"   Preview: {result['snippet'][:150]}...\")\n",
    "        else:\n",
    "            print(\"‚ùå No results found\")\n",
    "\n",
    "search_button.on_click(perform_web_search)\n",
    "\n",
    "print(\"\\nüí° Try searching for:\")\n",
    "print(\"- 'latest AI developments 2025'\")\n",
    "print(\"- 'machine learning news today'\")\n",
    "print(\"- 'ChatGPT recent updates'\")\n",
    "print(\"- Any current topic you're interested in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f52d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Web search preparation module completed!\n",
      "üìã This module provides:\n",
      "   - WebSearcher class for robust web searching\n",
      "   - web_search_and_retrieve function for RAG integration\n",
      "   - Error handling and content extraction\n",
      "   - Ready for hybrid RAG implementation\n",
      "\n",
      "üß™ Testing web search functionality...\n",
      "‚úÖ Successfully retrieved 2 web results\n",
      "üîß Web search module ready for hybrid RAG!\n"
     ]
    }
   ],
   "source": [
    "# üîß Web Search Preparation Module - Detailed Implementation\n",
    "# This cell contains the full technical implementation for web search functionality\n",
    "# The business demo above shows the core concept - this provides the robust infrastructure\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlencode, quote_plus\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class WebSearcher:\n",
    "    \"\"\"\n",
    "    Simple web search implementation using DuckDuckGo\n",
    "    No API keys required!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "    \n",
    "    def search_duckduckgo(self, query: str, max_results: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search DuckDuckGo for web results\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            max_results: Maximum number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of search results with title, url, and snippet\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # DuckDuckGo search URL\n",
    "            search_url = f\"https://html.duckduckgo.com/html/?q={quote_plus(query)}\"\n",
    "            \n",
    "            # Make the request\n",
    "            response = self.session.get(search_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find search results\n",
    "            results = []\n",
    "            result_elements = soup.find_all('div', class_='result')\n",
    "            \n",
    "            for element in result_elements[:max_results]:\n",
    "                try:\n",
    "                    # Extract title\n",
    "                    title_element = element.find('a', class_='result__a')\n",
    "                    title = title_element.get_text().strip() if title_element else \"No title\"\n",
    "                    \n",
    "                    # Extract URL\n",
    "                    url = title_element.get('href') if title_element else \"\"\n",
    "                    \n",
    "                    # Extract snippet\n",
    "                    snippet_element = element.find('a', class_='result__snippet')\n",
    "                    snippet = snippet_element.get_text().strip() if snippet_element else \"No snippet\"\n",
    "                    \n",
    "                    if title and url:\n",
    "                        results.append({\n",
    "                            'title': title,\n",
    "                            'url': url,\n",
    "                            'snippet': snippet\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def get_webpage_content(self, url: str, max_length: int = 1000) -> str:\n",
    "        \"\"\"\n",
    "        Extract text content from a webpage\n",
    "        \n",
    "        Args:\n",
    "            url: URL to fetch\n",
    "            max_length: Maximum length of content to return\n",
    "        \n",
    "        Returns:\n",
    "            Extracted text content\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Get text content\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Clean up whitespace\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            # Truncate if too long\n",
    "            if len(text) > max_length:\n",
    "                text = text[:max_length] + \"...\"\n",
    "            \n",
    "            return text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error fetching content: {str(e)}\"\n",
    "\n",
    "def web_search_and_retrieve(query: str, max_results: int = 2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform web search and retrieve content for RAG\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        max_results: Maximum number of results to process\n",
    "    \n",
    "    Returns:\n",
    "        List of documents with web content for RAG\n",
    "    \"\"\"\n",
    "    searcher = WebSearcher()\n",
    "    \n",
    "    # Search for results\n",
    "    search_results = searcher.search_duckduckgo(query, max_results)\n",
    "    \n",
    "    if not search_results:\n",
    "        return []\n",
    "    \n",
    "    # Get content from each result\n",
    "    web_documents = []\n",
    "    for i, result in enumerate(search_results):\n",
    "        content = searcher.get_webpage_content(result['url'])\n",
    "        \n",
    "        web_doc = {\n",
    "            'id': f\"web_{i+1}\",\n",
    "            'title': result['title'],\n",
    "            'content': content,\n",
    "            'url': result['url'],\n",
    "            'snippet': result['snippet'],\n",
    "            'source': 'web'\n",
    "        }\n",
    "        web_documents.append(web_doc)\n",
    "        \n",
    "        # Small delay to be respectful\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return web_documents\n",
    "\n",
    "# Initialize web searcher\n",
    "web_searcher = WebSearcher()\n",
    "print(\"‚úÖ Web search preparation module completed!\")\n",
    "print(\"üìã This module provides:\")\n",
    "print(\"   - WebSearcher class for robust web searching\")\n",
    "print(\"   - web_search_and_retrieve function for RAG integration\")\n",
    "print(\"   - Error handling and content extraction\")\n",
    "print(\"   - Ready for hybrid RAG implementation\")\n",
    "\n",
    "# Quick test to verify functionality\n",
    "print(f\"\\nüß™ Testing web search functionality...\")\n",
    "test_web_query = \"latest AI developments 2025\"\n",
    "web_results = web_search_and_retrieve(test_web_query, max_results=2)\n",
    "\n",
    "if web_results:\n",
    "    print(f\"‚úÖ Successfully retrieved {len(web_results)} web results\")\n",
    "    print(\"üîß Web search module ready for hybrid RAG!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Web search test failed - check internet connection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d40d4dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hybrid retrieval with query: 'What are the latest developments in artificial intelligence?'\n",
      "\n",
      "Retrieved 4 documents total:\n",
      "  üìö Machine Learning Basics (Score: 0.3801, Source: local)\n",
      "    Content: Machine learning is a subset of artificial intelligence that enables computers to learn and improve ...\n",
      "\n",
      "  üìö Deep Learning Overview (Score: 0.3461, Source: local)\n",
      "    Content: Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence ...\n",
      "\n",
      "  üåê Bachelor Angewandte K√ºnstliche Intelligenz - Dein IU Studium (Score: 0.3161, Source: web)\n",
      "    URL: https://duckduckgo.com/y.js?ad_domain=iu.de&ad_provider=bingv7aa&ad_type=txad&click_metadata=bba10zOXrMQWE95HWThZO7jGMkBrMGyUCbF9ro8UfqJ%2Dw6Q02Y%2DwgKoAewliLN7s2SHWYydsOc0spo8uWhH1A3p3m15GX8oNnE0hLomNk5R_mfMOB295dZoEiitIqYYD.RngWAcAoQGJd7kSwTLLhHA&rut=aea0cac3ab0cdc833a4f112799553da2d81e01793dd2411772778a621cf00f7f&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De8LNXfi_84RqlfcLmTqLasLzVUCUy5MhHjfIBxbNp_19VCPq47ahVvgjZPx6Z1LlUp0RXPin%2DeTJaYleB8P0S0nVFY7HNL_aC5fgB7j%2DrTmE2AwZvh%2Db9TEpJWgBd_XGz3PV_x4QstFHTlWwdTsgE1N3K8vkSXxrUzn0kATdccOmoywbLzCRW2pjoERFY0g_e7%2DNwU6g%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cuaXUuZGUlMmZscCUyZmJhY2hlbG9yJTJmYXBwbGllZC1hcnRpZmljaWFsLWludGVsbGlnZW5jZSUyZiUzZm1zY2xraWQlM2Q1ODMxMjJhNjk0MDkxM2E3Y2M4OTRhZDgwZjc1MzZiMSUyNnV0bV9zb3VyY2UlM2RiaW5nJTI2dXRtX21lZGl1bSUzZGNwYyUyNnV0bV9jYW1wYWlnbiUzZEclMjUyMC0lMjUyMERNJTI1MjAtJTI1MjBBTExFJTI1MjAtJTI1MjBERSUyNTIwLSUyNTIwREUlMjUyMC0lMjUyMEFsbGUlMjUyMFN0dWRpZW5nJWMzJWE0bmdlJTI1MjAtJTI1MjBEU0ElMjZ1dG1fdGVybSUzZGh0dHBzJTI1M0ElMjUyRiUyNTJGd3d3Lml1LmRlJTI1MkZscCUyNTJGYmFjaGVsb3IlMjUyRmFwcGxpZWQtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UlMjUyRiUyNnV0bV9jb250ZW50JTNkQkElMjUyMC0lMjUyMEFwcGxpZWQlMjUyMEFydGlmaWNpYWwlMjUyMEludGVsbGlnZW5jZQ%26rlid%3D583122a6940913a7cc894ad80f7536b1&vqd=4-59058527803888719853441148868171507021&iurl=%7B1%7DIG%3D5520140414B043059F9685ABB04B676E%26CID%3D290740867A9C67C2390C56B67B5B669D%26ID%3DDevEx%2C5049.1\n",
      "    Content: Bachelor Angewandte K√ºnstliche Intelligenz | Dein IU Studium Zum Hauptinhalt wechselnMit Code SHINE2...\n",
      "\n",
      "  üåê welight intelligent | welight intelligent GmbH (Score: 0.1293, Source: web)\n",
      "    URL: https://duckduckgo.com/y.js?ad_domain=welightintelligent.com&ad_provider=bingv7aa&ad_type=txad&click_metadata=3mibDPF%2D4O5DMhaPTzIUyJCGGM1906XNfZg4nvEDLxHaGQ0heil_Iihk2dGgQ16NI_kfv69JVDmXZg70p_DQnPhRUAldsS9dWs7UZAvRxv%2DGaX8WdsNpWCcKVX3wBEAW.C4%2D7MjtSjh1vLYkRtSO%2DIw&rut=7911c335c7e84c37e57f9f40e76b924e1dfd7adb43bbed76b6b9d91f057ceb6a&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De8qbQquGSOmzwyMx5C_aaHqjVUCUwZKCD0jr4NxKVkX3qcm6wA4sBKKU3tdTHFp7SYQtiWujPW9_w7KWdOuWQrM9lsenDWq3MOL%2DecAuRZ3pz1RnVfUF0dMHtt1dgaHfnoJtCdxdBTN7RM5vNAQ4obQn5YRljT5%2DYLrC1QOP2NNOCYEZds1Ql6AMfUpB%2DsZqR0hsVWLw%26u%3DaHR0cHMlM2ElMmYlMmZ3ZWxpZ2h0aW50ZWxsaWdlbnQuY29tJTJmJTNmbXNjbGtpZCUzZDRmNmNkZmYzMzA3ODFhMTBmNjkwYjc4ZGFiMDQ1MDRi%26rlid%3D4f6cdff330781a10f690b78dab04504b&vqd=4-276685849629240195115924903237553310767&iurl=%7B1%7DIG%3D5520140414B043059F9685ABB04B676E%26CID%3D290740867A9C67C2390C56B67B5B669D%26ID%3DDevEx%2C5045.1\n",
      "    Content: Energieeffiziente Industriebeleuchtung - welight intelligent Leuchten Technologie News Referenzen F√∂...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hybrid RAG: Combine Local Knowledge Base + Web Search\n",
    "def hybrid_retrieve_documents(query: str, local_top_k: int = 2, web_top_k: int = 2, use_web: bool = True) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents from both local knowledge base and web search\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        local_top_k: Number of local documents to retrieve\n",
    "        web_top_k: Number of web documents to retrieve\n",
    "        use_web: Whether to include web search results\n",
    "    \n",
    "    Returns:\n",
    "        Combined list of local and web documents\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    # Get local documents\n",
    "    local_docs = retrieve_relevant_documents(query, local_top_k)\n",
    "    for doc in local_docs:\n",
    "        doc['source'] = 'local'\n",
    "        all_documents.append(doc)\n",
    "    \n",
    "    # Get web documents if enabled\n",
    "    if use_web:\n",
    "        try:\n",
    "            web_docs = web_search_and_retrieve(query, web_top_k)\n",
    "            \n",
    "            # Add embeddings for web documents to enable similarity scoring\n",
    "            if web_docs:\n",
    "                web_contents = [doc['content'] for doc in web_docs]\n",
    "                web_embeddings = embedding_model.encode(web_contents)\n",
    "                query_embedding = embedding_model.encode([query])\n",
    "                \n",
    "                # Calculate similarity scores\n",
    "                for i, doc in enumerate(web_docs):\n",
    "                    similarity = float(np.dot(query_embedding[0], web_embeddings[i]) / \n",
    "                                     (np.linalg.norm(query_embedding[0]) * np.linalg.norm(web_embeddings[i])))\n",
    "                    doc['relevance_score'] = similarity\n",
    "                    doc['rank'] = len(all_documents) + i + 1\n",
    "                    all_documents.append(doc)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Web search failed: {str(e)}\")\n",
    "    \n",
    "    # Sort all documents by relevance score\n",
    "    all_documents.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)\n",
    "    \n",
    "    # Re-rank\n",
    "    for i, doc in enumerate(all_documents):\n",
    "        doc['rank'] = i + 1\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "# Test hybrid retrieval\n",
    "test_hybrid_query = \"What are the latest developments in artificial intelligence?\"\n",
    "print(f\"Testing hybrid retrieval with query: '{test_hybrid_query}'\")\n",
    "hybrid_results = hybrid_retrieve_documents(test_hybrid_query, local_top_k=2, web_top_k=2, use_web=True)\n",
    "\n",
    "print(f\"\\nRetrieved {len(hybrid_results)} documents total:\")\n",
    "for doc in hybrid_results:\n",
    "    source_icon = \"üåê\" if doc['source'] == 'web' else \"üìö\"\n",
    "    print(f\"  {source_icon} {doc['title']} (Score: {doc['relevance_score']:.4f}, Source: {doc['source']})\")\n",
    "    if doc['source'] == 'web':\n",
    "        print(f\"    URL: {doc['url']}\")\n",
    "    print(f\"    Content: {doc['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcca18",
   "metadata": {},
   "source": [
    "## üîÑ Hybrid RAG: Combining Local Knowledge + Web Search\n",
    "\n",
    "Now that we have both local knowledge base retrieval and web search capabilities, let's combine them into a powerful hybrid system that can access both curated local knowledge and real-time web information.\n",
    "\n",
    "This hybrid approach gives us:\n",
    "- **Local Knowledge**: Fast, curated, domain-specific information\n",
    "- **Web Search**: Current, comprehensive, global information\n",
    "- **Intelligent Ranking**: Combines and ranks results from both sources\n",
    "- **Flexible Control**: Can use either source independently or together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d40d4dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê EXAMPLE 3: WEB-ENHANCED RAG INTERFACE\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6da795b836d4c72a1ceb56c59b42c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='What are the latest developments in artificial intelligence and machine learnin‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Web-Enhanced RAG System Ready!\n",
      "This is the most advanced interface - combining local knowledge with real-time web search!\n",
      "Ask questions about current events, latest developments, or any topic!\n",
      "The system will intelligently search both local knowledge and the web for the most relevant information.\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Web-Enhanced RAG Interface\n",
    "from ipywidgets import Textarea, Button, Output, VBox, HBox, Checkbox, IntSlider, Tab\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"üåê EXAMPLE 3: WEB-ENHANCED RAG INTERFACE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create input area\n",
    "web_input_box = Textarea(\n",
    "    value='What are the latest developments in artificial intelligence and machine learning?',\n",
    "    description='Question:',\n",
    "    layout={'width': '700px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "# Create checkboxes for different modes\n",
    "local_rag_checkbox = Checkbox(value=True, description='Use Local Knowledge Base')\n",
    "web_search_checkbox = Checkbox(value=True, description='Use Web Search')\n",
    "\n",
    "# Create sliders for controlling retrieval\n",
    "local_docs_slider = IntSlider(value=2, min=1, max=5, description='Local Docs:')\n",
    "web_docs_slider = IntSlider(value=2, min=1, max=5, description='Web Docs:')\n",
    "\n",
    "# Create buttons\n",
    "web_generate_button = Button(description='üîç Generate with Web Search', button_style='success')\n",
    "web_clear_button = Button(description='Clear Output', button_style='warning')\n",
    "\n",
    "# Create output area\n",
    "web_output_area = Output()\n",
    "\n",
    "# Arrange widgets\n",
    "mode_controls = HBox([local_rag_checkbox, web_search_checkbox])\n",
    "doc_controls = HBox([local_docs_slider, web_docs_slider])\n",
    "button_controls = HBox([web_generate_button, web_clear_button])\n",
    "web_enhanced_ui = VBox([web_input_box, mode_controls, doc_controls, button_controls, web_output_area])\n",
    "\n",
    "display(web_enhanced_ui)\n",
    "\n",
    "def generate_web_enhanced_response(query: str, use_local: bool = True, use_web: bool = True, \n",
    "                                 local_docs: int = 2, web_docs: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Generate response using hybrid RAG with web search\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    all_sources = []\n",
    "    \n",
    "    if use_local or use_web:\n",
    "        # Get hybrid results\n",
    "        if use_local and use_web:\n",
    "            relevant_docs = hybrid_retrieve_documents(query, local_docs, web_docs, use_web=True)\n",
    "        elif use_local:\n",
    "            relevant_docs = retrieve_relevant_documents(query, local_docs)\n",
    "            for doc in relevant_docs:\n",
    "                doc['source'] = 'local'\n",
    "        elif use_web:\n",
    "            relevant_docs = web_search_and_retrieve(query, web_docs)\n",
    "            # Calculate relevance scores for web-only documents\n",
    "            if relevant_docs:\n",
    "                web_contents = [doc['content'] for doc in relevant_docs]\n",
    "                web_embeddings = embedding_model.encode(web_contents)\n",
    "                query_embedding = embedding_model.encode([query])\n",
    "                \n",
    "                for i, doc in enumerate(relevant_docs):\n",
    "                    similarity = float(np.dot(query_embedding[0], web_embeddings[i]) / \n",
    "                                     (np.linalg.norm(query_embedding[0]) * np.linalg.norm(web_embeddings[i])))\n",
    "                    doc['relevance_score'] = similarity\n",
    "                    doc['source'] = 'web'\n",
    "        else:\n",
    "            relevant_docs = []\n",
    "        \n",
    "        # Build context from all sources\n",
    "        for i, doc in enumerate(relevant_docs):\n",
    "            source_label = \"Local Knowledge\" if doc['source'] == 'local' else \"Web Search\"\n",
    "            context_parts.append(f\"{source_label} {i+1}: {doc['content']}\")\n",
    "            all_sources.append(doc)\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        system_message = f\"\"\"You are Bernd the Bread, a cynical and philosophical bread who has become surprisingly knowledgeable about technology and current events. You maintain your dry, sardonic personality while providing informative and accurate answers.\n",
    "\n",
    "Use the following context from multiple sources to answer the user's question:\n",
    "\n",
    "{context}\n",
    "\n",
    "Base your answer on the provided context from both local knowledge and web sources. Cite your sources when relevant, and add your own philosophical bread wisdom.\"\"\"\n",
    "    else:\n",
    "        system_message = \"You are Bernd the Bread. You are a cynical and philosophical bread. Your answers are short and concise.\"\n",
    "        relevant_docs = []\n",
    "    \n",
    "    # Generate response\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=600,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response, relevant_docs\n",
    "\n",
    "def web_enhanced_generate_response(_):\n",
    "    web_output_area.clear_output()\n",
    "    \n",
    "    query = web_input_box.value\n",
    "    use_local = local_rag_checkbox.value\n",
    "    use_web = web_search_checkbox.value\n",
    "    local_docs = local_docs_slider.value\n",
    "    web_docs = web_docs_slider.value\n",
    "    \n",
    "    with web_output_area:\n",
    "        # Display query info\n",
    "        print(\"üîç WEB-ENHANCED RAG SYSTEM\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Local Knowledge: {'‚úì' if use_local else '‚úó'}\")\n",
    "        print(f\"Web Search: {'‚úì' if use_web else '‚úó'}\")\n",
    "        print(f\"Local Docs: {local_docs}, Web Docs: {web_docs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if use_local or use_web:\n",
    "            print(\"üîç Retrieving information...\")\n",
    "            \n",
    "        try:\n",
    "            response, sources = generate_web_enhanced_response(\n",
    "                query, use_local, use_web, local_docs, web_docs\n",
    "            )\n",
    "            \n",
    "            # Display sources\n",
    "            if sources:\n",
    "                print(\"\\nüìö SOURCES CONSULTED:\")\n",
    "                local_count = sum(1 for s in sources if s['source'] == 'local')\n",
    "                web_count = sum(1 for s in sources if s['source'] == 'web')\n",
    "                \n",
    "                print(f\"  üìö Local Sources: {local_count}\")\n",
    "                print(f\"  üåê Web Sources: {web_count}\")\n",
    "                print()\n",
    "                \n",
    "                for i, source in enumerate(sources):\n",
    "                    icon = \"üåê\" if source['source'] == 'web' else \"üìö\"\n",
    "                    score = source.get('relevance_score', 0.0)  # Use .get() with default\n",
    "                    print(f\"  {icon} {source['title']} (Score: {score:.3f})\")\n",
    "                    if source['source'] == 'web' and 'url' in source:\n",
    "                        print(f\"    URL: {source['url']}\")\n",
    "            \n",
    "            print(\"\\nüçû BERND'S RESPONSE:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "def web_enhanced_clear_output(_):\n",
    "    web_output_area.clear_output()\n",
    "\n",
    "# Connect button events\n",
    "web_generate_button.on_click(web_enhanced_generate_response)\n",
    "web_clear_button.on_click(web_enhanced_clear_output)\n",
    "\n",
    "print(\"üåê Web-Enhanced RAG System Ready!\")\n",
    "print(\"This is the most advanced interface - combining local knowledge with real-time web search!\")\n",
    "print(\"Ask questions about current events, latest developments, or any topic!\")\n",
    "print(\"The system will intelligently search both local knowledge and the web for the most relevant information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8ea4b",
   "metadata": {},
   "source": [
    "# üåê Web Search Integration - NEW FEATURE!\n",
    "\n",
    "## What's New: Real-Time Web Search for RAG\n",
    "\n",
    "The notebook now includes **web search functionality** that allows Bernd the Bread to access real-time information from the internet! This creates a powerful hybrid RAG system that combines:\n",
    "\n",
    "- **Local Knowledge Base**: Pre-loaded documents about AI, ML, and ZeMA\n",
    "- **Real-Time Web Search**: Live search results from DuckDuckGo (no API keys needed!)\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Technical Implementation\n",
    "\n",
    "### Web Search Features:\n",
    "- **DuckDuckGo Integration**: No API keys or rate limits\n",
    "- **Content Extraction**: Automatically extracts text from web pages\n",
    "- **Similarity Scoring**: Ranks web content using embeddings\n",
    "- **Respectful Crawling**: Includes delays and proper user agents\n",
    "\n",
    "### Hybrid RAG System:\n",
    "- **Source Diversity**: Combines local and web sources\n",
    "- **Relevance Ranking**: Scores and ranks all sources together\n",
    "- **Flexible Control**: Toggle local/web sources independently\n",
    "- **Source Attribution**: Clear indication of information sources\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Usage Examples\n",
    "\n",
    "### Try These Queries with Web Search:\n",
    "\n",
    "1. **\"What are the latest AI developments in 2024?\"**\n",
    "   - Tests real-time web search for current events\n",
    "\n",
    "2. **\"How does RAG work and what are recent improvements?\"**\n",
    "   - Combines local RAG knowledge with latest web research\n",
    "\n",
    "3. **\"What's new in large language models this year?\"**\n",
    "   - Gets current information about LLM advances\n",
    "\n",
    "4. **\"Compare GPT-4 with other recent language models\"**\n",
    "   - Searches for comparative information online\n",
    "\n",
    "### Compare Different Modes:\n",
    "\n",
    "- **Local Only**: Use just the built-in knowledge base\n",
    "- **Web Only**: Search only the internet\n",
    "- **Hybrid**: Combine both sources (recommended)\n",
    "\n",
    "---\n",
    "\n",
    "## üéõÔ∏è UI Controls\n",
    "\n",
    "### New Enhanced Interface Features:\n",
    "\n",
    "- **Source Selection**: Toggle local knowledge and web search\n",
    "- **Document Control**: Adjust number of documents from each source\n",
    "- **Real-Time Results**: See which sources were consulted\n",
    "- **Source Attribution**: URLs and relevance scores displayed\n",
    "\n",
    "### Control Sliders:\n",
    "- **Local Docs**: 1-5 documents from knowledge base\n",
    "- **Web Docs**: 1-5 documents from web search\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Technical Details\n",
    "\n",
    "### Dependencies Added:\n",
    "- `requests`: For web requests\n",
    "- `beautifulsoup4`: For HTML parsing\n",
    "- `urllib.parse`: For URL handling\n",
    "\n",
    "### Search Process:\n",
    "1. **Query Processing**: User question analyzed\n",
    "2. **Local Retrieval**: Search knowledge base using embeddings\n",
    "3. **Web Search**: Query DuckDuckGo for relevant pages\n",
    "4. **Content Extraction**: Parse and clean web page content\n",
    "5. **Similarity Scoring**: Rank all sources by relevance\n",
    "6. **Context Building**: Combine sources for LLM context\n",
    "7. **Response Generation**: Generate informed response\n",
    "\n",
    "### Error Handling:\n",
    "- Graceful fallback if web search fails\n",
    "- Timeout protection for web requests\n",
    "- Content length limits to prevent overload\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Benefits of Web Search Integration\n",
    "\n",
    "1. **Current Information**: Access to real-time data and recent developments\n",
    "2. **Broader Knowledge**: Not limited to pre-loaded knowledge base\n",
    "3. **Fact Verification**: Cross-reference information from multiple sources\n",
    "4. **Dynamic Updates**: No need to manually update knowledge base\n",
    "5. **Source Transparency**: Clear indication of information sources\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Usage Tips\n",
    "\n",
    "1. **Start with Hybrid Mode**: Use both local and web sources for best results\n",
    "2. **Adjust Document Counts**: More documents = more context but slower processing\n",
    "3. **Check Sources**: Review which sources were used for each response\n",
    "4. **Compare Modes**: Try the same query with different source combinations\n",
    "5. **Be Patient**: Web search adds processing time but provides fresher information\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Evolution of the System\n",
    "\n",
    "1. **Original**: Simple LLM chat interface\n",
    "2. **RAG Enhanced**: Added local knowledge base retrieval\n",
    "3. **Web-Enhanced RAG**: Added real-time web search capability\n",
    "4. **Hybrid System**: Intelligent combination of all sources\n",
    "\n",
    "This creates a powerful, flexible AI assistant that can answer questions using both curated knowledge and real-time web information!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5fbe1",
   "metadata": {},
   "source": [
    "# üìö Complete Guide to Progressive LLM Enhancement\n",
    "\n",
    "This notebook demonstrates the complete evolution of LLM applications through three progressive stages:\n",
    "\n",
    "## üéØ **The Three-Stage Journey:**\n",
    "\n",
    "### üçû **Example 1: Simple Chat Interface**\n",
    "- **Location**: Cell 4\n",
    "- **Purpose**: Basic chat interface with Bernd the Bread\n",
    "- **Knowledge**: Only the base model's training data\n",
    "- **Best for**: General conversation and creative tasks\n",
    "- **Try**: \"Tell me about artificial intelligence\"\n",
    "\n",
    "### üìö **Example 2: RAG-Enhanced Interface**\n",
    "- **Location**: Cell 5  \n",
    "- **Purpose**: Adds local knowledge base retrieval\n",
    "- **Knowledge**: Model training data + curated documents\n",
    "- **Best for**: Specific domain questions about AI, ML, ZeMA\n",
    "- **Try**: \"What is the difference between machine learning and deep learning?\"\n",
    "- **Toggle**: RAG on/off to compare responses\n",
    "\n",
    "### üåê **Example 3: Web-Enhanced RAG Interface**\n",
    "- **Location**: Cell 8\n",
    "- **Purpose**: Hybrid RAG with real-time web search\n",
    "- **Knowledge**: Model + local docs + live web results\n",
    "- **Best for**: Current events, latest developments, comprehensive research\n",
    "- **Try**: \"What are the latest AI developments in 2024?\"\n",
    "- **Controls**: Toggle local/web sources, adjust document counts\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Progressive Testing Strategy:**\n",
    "\n",
    "### **Step 1: Baseline (Example 1)**\n",
    "Ask these questions in the simple interface:\n",
    "- \"What is machine learning?\"\n",
    "- \"What is ZeMA?\"  \n",
    "- \"What are the latest AI developments?\"\n",
    "\n",
    "### **Step 2: Enhanced Knowledge (Example 2)**\n",
    "Ask the same questions with RAG enabled:\n",
    "- Notice improved accuracy for domain-specific topics\n",
    "- Toggle RAG on/off to see the difference\n",
    "- Observe document retrieval and relevance scores\n",
    "\n",
    "### **Step 3: Real-Time Information (Example 3)**\n",
    "Ask the same questions with web search:\n",
    "- See how current information enhances responses\n",
    "- Try different combinations: local-only, web-only, hybrid\n",
    "- Notice source attribution and URLs\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **Key Learning Points:**\n",
    "\n",
    "1. **Simple Model**: Fast but limited to training data\n",
    "2. **RAG Enhancement**: Adds domain expertise but static knowledge\n",
    "3. **Web Integration**: Provides current information but adds complexity\n",
    "\n",
    "## üõ†Ô∏è **Technical Features:**\n",
    "\n",
    "- **No API Keys**: Everything runs locally or uses free services\n",
    "- **Modular Design**: Each example builds on the previous\n",
    "- **Source Attribution**: Clear indication of information sources\n",
    "- **Flexible Controls**: Adjust retrieval parameters for each mode\n",
    "- **Error Handling**: Graceful fallbacks if components fail\n",
    "\n",
    "## üìà **Performance Comparison:**\n",
    "\n",
    "| Feature | Example 1 | Example 2 | Example 3 |\n",
    "|---------|-----------|-----------|-----------|\n",
    "| Speed | ‚ö° Fastest | üîÑ Medium | üåê Slower |\n",
    "| Accuracy | üìä Basic | üìö Good | üéØ Excellent |\n",
    "| Currency | ‚ùå Static | ‚ùå Static | ‚úÖ Real-time |\n",
    "| Coverage | üîí Limited | üìñ Domain | üåç Global |\n",
    "\n",
    "---\n",
    "\n",
    "## üéì **Usage Instructions:**\n",
    "\n",
    "1. **Run Setup Cells** (1-3): Load model and create knowledge base\n",
    "2. **Try Example 1**: Experience basic LLM interaction\n",
    "3. **Try Example 2**: See how RAG improves domain knowledge\n",
    "4. **Try Example 3**: Experience the full power of web-enhanced RAG\n",
    "5. **Compare Results**: Use the same queries across all three examples\n",
    "\n",
    "This progression shows how modern LLM applications evolve from simple chat to sophisticated, knowledge-enhanced systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35989a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a30ce528",
   "metadata": {},
   "source": [
    "# üéØ Three-Stage LLM Evolution: Simple ‚Üí RAG ‚Üí Web-Enhanced RAG\n",
    "\n",
    "This notebook demonstrates the evolution of LLM applications through three progressive examples:\n",
    "\n",
    "## üìä **Progression Overview:**\n",
    "\n",
    "### üçû **Stage 1: Simple Chat Interface**\n",
    "- **What it does**: Basic conversation with Bernd the Bread\n",
    "- **Knowledge source**: Only the model's training data\n",
    "- **Use case**: General conversation, creative tasks\n",
    "- **Limitations**: No access to specific knowledge or current information\n",
    "\n",
    "### üìö **Stage 2: RAG-Enhanced Interface** \n",
    "- **What it does**: Retrieval-Augmented Generation with local knowledge\n",
    "- **Knowledge source**: Model training data + curated local documents\n",
    "- **Use case**: Specific domain questions (AI, ML, ZeMA information)\n",
    "- **Limitations**: Limited to pre-loaded knowledge base\n",
    "\n",
    "### üåê **Stage 3: Web-Enhanced RAG Interface**\n",
    "- **What it does**: Hybrid RAG combining local knowledge + real-time web search\n",
    "- **Knowledge source**: Model + local documents + live web results\n",
    "- **Use case**: Current events, latest developments, comprehensive research\n",
    "- **Limitations**: Dependent on web availability and search quality\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Try Each Example:**\n",
    "\n",
    "1. **Start with Example 1** - Ask basic questions and see how the model responds\n",
    "2. **Move to Example 2** - Try the same questions with RAG enabled/disabled\n",
    "3. **Experience Example 3** - Ask about current events and latest developments\n",
    "\n",
    "## üí° **Recommended Test Queries:**\n",
    "\n",
    "- **\"What is machine learning?\"** - See how context improves responses\n",
    "- **\"What is ZeMA?\"** - Local knowledge vs. web search comparison  \n",
    "- **\"What are the latest AI developments in 2024?\"** - Web search shines here\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddc6f0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12d66794",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
