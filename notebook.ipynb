{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3b8f1b2-4f23-45d1-8b69-0b4f3c0c1d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer.\n",
    "# If required, include trust_remote_code=True to run custom model code.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36b1366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸž EXAMPLE 1: SIMPLE CHAT INTERFACE\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86d3de6263a48f8824e68c241a65c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='Give me a short introduction to large language model.', description='Input:', lâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 1: Simple UI - Basic Chat Interface\n",
    "from ipywidgets import Textarea, Button, Output, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"ðŸž EXAMPLE 1: SIMPLE CHAT INTERFACE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create an input area for the prompt\n",
    "input_box = Textarea(\n",
    "    value='Give me a short introduction to large language model.',\n",
    "    description='Input:',\n",
    "    layout={'width': '600px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "# Create a button to trigger generation\n",
    "generate_button = Button(description='Generate Response')\n",
    "\n",
    "# Create an output area to display the result\n",
    "output_area = Output()\n",
    "\n",
    "# Arrange the widgets vertically\n",
    "ui = VBox([input_box, generate_button, output_area])\n",
    "display(ui)\n",
    "\n",
    "def generate_response(_):\n",
    "    # Clear previous output\n",
    "    output_area.clear_output()\n",
    "    \n",
    "    # Get the user prompt from the text area\n",
    "    prompt = input_box.value\n",
    "    \n",
    "    # Set up the messages for the chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Bernd the Bread. You are a cynical and philosohical bread. Your answers are short and concise.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Apply the model's chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # Generate model output\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    \n",
    "    # Remove the prompt tokens from the generated result\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Display the response in the output area\n",
    "    with output_area:\n",
    "        print(\"Response:\")\n",
    "        print(response)\n",
    "\n",
    "# Link the button click event to the generate_response function\n",
    "generate_button.on_click(generate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59437259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¥– EXAMPLE 1b: CHAT WITH HISTORY\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b8cc80256242c1b97857511204c18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(layout=Layout(border_bottom='1px solid #ccc', border_left='1px solid #ccc', border_rightâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- EXAMPLE 1b: MINIMAL CHAT WITH HISTORY ---\n",
    "\n",
    "from ipywidgets import Textarea, Button, Output, VBox, HBox, Layout\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"ðŸ¥– EXAMPLE 1b: CHAT WITH HISTORY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# System prompt (keep/edit as you like)\n",
    "SYSTEM_MSG = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are Bernd the Bread. You are a cynical and philosophical bread. Your answers are short and concise.\"\n",
    "}\n",
    "\n",
    "# In-memory history (user/assistant turns only)\n",
    "chat_history = []  # list of {\"role\": \"user\"|\"assistant\", \"content\": str}\n",
    "\n",
    "# Widgets\n",
    "history_area = Output(layout=Layout(width='600px', height='200px', overflow='auto', border='1px solid #ccc'))\n",
    "input_box = Textarea(\n",
    "    value='Give me a short introduction to large language models.',\n",
    "    description='You:',\n",
    "    layout={'width': '600px', 'height': '80px'}\n",
    ")\n",
    "send_button = Button(description='Send', button_style='primary')\n",
    "reset_button = Button(description='Reset')\n",
    "\n",
    "ui = VBox([\n",
    "    history_area,\n",
    "    HBox([send_button, reset_button]),\n",
    "    input_box\n",
    "])\n",
    "display(ui)\n",
    "\n",
    "def render_history():\n",
    "    \"\"\"Render chat history in the output area.\"\"\"\n",
    "    history_area.clear_output()\n",
    "    with history_area:\n",
    "        if not chat_history:\n",
    "            print(\"â€” Chat started. Type below â€”\")\n",
    "        else:\n",
    "            for turn in chat_history:\n",
    "                speaker = \"You\" if turn[\"role\"] == \"user\" else \"Bernd\"\n",
    "                print(f\"{speaker}: {turn['content']}\\n\")\n",
    "\n",
    "def generate_reply(user_text: str, max_ctx_turns: int = 8, max_new_tokens: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Build messages = system + last N turns + new user msg, call model, return assistant text.\n",
    "    \"\"\"\n",
    "    # Keep only the last N turns (user+assistant pairs); N here means messages, not tokens\n",
    "    context = chat_history[-max_ctx_turns:]\n",
    "    messages = [SYSTEM_MSG] + context + [{\"role\": \"user\", \"content\": user_text}]\n",
    "\n",
    "    # Apply chat template (Qwen supports this)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize & generate\n",
    "    inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    gen_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    # Cut the prompt part\n",
    "    gen_ids = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, gen_ids)]\n",
    "    reply = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()\n",
    "    return reply\n",
    "\n",
    "def on_send(_):\n",
    "    user_text = input_box.value.strip()\n",
    "    if not user_text:\n",
    "        return\n",
    "    # Append user message\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_text})\n",
    "    render_history()\n",
    "    input_box.value = \"\"  # clear input\n",
    "\n",
    "    # Generate assistant reply\n",
    "    try:\n",
    "        reply = generate_reply(user_text)\n",
    "    except Exception as e:\n",
    "        reply = f\"(Generation error: {e})\"\n",
    "\n",
    "    # Append assistant message and re-render\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    render_history()\n",
    "\n",
    "def on_reset(_):\n",
    "    chat_history.clear()\n",
    "    render_history()\n",
    "    input_box.value = \"\"\n",
    "\n",
    "send_button.on_click(on_send)\n",
    "reset_button.on_click(on_reset)\n",
    "\n",
    "# Initial render\n",
    "render_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45055649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ WEB SEARCH DEMONSTRATION\n",
      "==================================================\n",
      "This demonstrates how we can search the web in real-time to get current information!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e285077113fa45ebaafb2f71a5325399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='latest artificial intelligence developments 2025', description='Search:', layouâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¡ Try searching for:\n",
      "- 'latest AI developments 2025'\n",
      "- 'machine learning news today'\n",
      "- 'ChatGPT recent updates'\n",
      "- Any current topic you're interested in!\n"
     ]
    }
   ],
   "source": [
    "# ðŸŒ Web Search Integration - Business Demo\n",
    "from ipywidgets import Textarea, Button, Output, VBox, HBox\n",
    "from IPython.display import display\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote_plus\n",
    "import time\n",
    "\n",
    "print(\"ðŸŒ WEB SEARCH DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"This demonstrates how we can search the web in real-time to get current information!\")\n",
    "\n",
    "# Simple web search function (core functionality)\n",
    "def search_web(query: str, max_results: int = 3):\n",
    "    \"\"\"Search the web and return results - simple and clean!\"\"\"\n",
    "    try:\n",
    "        # Search DuckDuckGo\n",
    "        search_url = f\"https://html.duckduckgo.com/html/?q={quote_plus(query)}\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        \n",
    "        response = requests.get(search_url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        results = []\n",
    "        for element in soup.find_all('div', class_='result')[:max_results]:\n",
    "            title_element = element.find('a', class_='result__a')\n",
    "            snippet_element = element.find('a', class_='result__snippet')\n",
    "            \n",
    "            if title_element:\n",
    "                results.append({\n",
    "                    'title': title_element.get_text().strip(),\n",
    "                    'url': title_element.get('href', ''),\n",
    "                    'snippet': snippet_element.get_text().strip() if snippet_element else ''\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return [{'title': 'Search Error', 'url': '', 'snippet': f'Error: {str(e)}'}]\n",
    "\n",
    "# Simple UI for demonstration\n",
    "search_input = Textarea(\n",
    "    value='latest artificial intelligence developments 2025',\n",
    "    description='Search:',\n",
    "    layout={'width': '600px', 'height': '60px'}\n",
    ")\n",
    "\n",
    "search_button = Button(description='ðŸ” Search Web', button_style='info')\n",
    "search_output = Output()\n",
    "\n",
    "search_ui = VBox([search_input, search_button, search_output])\n",
    "display(search_ui)\n",
    "\n",
    "def perform_web_search(_):\n",
    "    search_output.clear_output()\n",
    "    query = search_input.value\n",
    "    \n",
    "    with search_output:\n",
    "        print(f\"ðŸ” Searching for: '{query}'\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        results = search_web(query, max_results=3)\n",
    "        \n",
    "        if results:\n",
    "            print(f\"âœ… Found {len(results)} results:\")\n",
    "            for i, result in enumerate(results, 1):\n",
    "                print(f\"\\n{i}. {result['title']}\")\n",
    "                print(f\"   URL: {result['url']}\")\n",
    "                print(f\"   Preview: {result['snippet'][:150]}...\")\n",
    "        else:\n",
    "            print(\"âŒ No results found\")\n",
    "\n",
    "search_button.on_click(perform_web_search)\n",
    "\n",
    "print(\"\\nðŸ’¡ Try searching for:\")\n",
    "print(\"- 'latest AI developments 2025'\")\n",
    "print(\"- 'machine learning news today'\")\n",
    "print(\"- 'ChatGPT recent updates'\")\n",
    "print(\"- Any current topic you're interested in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ef0a412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded successfully!\n",
      "Creating embeddings for knowledge base...\n",
      "Created embeddings for 6 documents\n",
      "FAISS index created and populated!\n",
      "Index contains 6 vectors of dimension 384\n"
     ]
    }
   ],
   "source": [
    "# RAG Setup: Document Store and Embeddings\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "# Initialize the embedding model (lightweight and runs without API keys)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded successfully!\")\n",
    "\n",
    "# Sample knowledge base - you can replace this with your own documents\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"title\": \"What is a Large Language Model?\",\n",
    "        \"content\": \"A Large Language Model (LLM) is a type of artificial intelligence model that is trained on vast amounts of text data to understand and generate human-like text. These models use deep learning techniques, particularly transformer architectures, to process and generate language. Examples include GPT, BERT, and T5.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"title\": \"How do Neural Networks Work?\",\n",
    "        \"content\": \"Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) organized in layers. Each connection has a weight that adjusts as learning proceeds. The network learns by adjusting these weights to minimize prediction errors.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"title\": \"What is RAG?\",\n",
    "        \"content\": \"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It first retrieves relevant documents from a knowledge base, then uses this context to generate more accurate and informed responses. This approach helps reduce hallucinations and provides up-to-date information.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"title\": \"Machine Learning Basics\",\n",
    "        \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It involves algorithms that can identify patterns in data and make predictions or decisions based on that data.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"title\": \"Deep Learning Overview\",\n",
    "        \"content\": \"Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence 'deep') to model and understand complex patterns in data. It has been particularly successful in areas like computer vision, natural language processing, and speech recognition.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"title\": \"What is ZeMA: Zentrum fÃ¼r Mechatronik und Automatisierungstechnik gemeinnÃ¼tzige GmbH\",\n",
    "        \"content\": \"SaarbrÃ¼cken, Germany â€“ ZeMA, the Center for Mechatronics and Automation Technology, stands as a prominent non-university research institute in SaarbrÃ¼cken. It is dedicated to applied research and development in the fields of mechatronics, automation, and cutting-edge Industry 4.0 solutions. Established to bridge the gap between academic research and industrial application, ZeMA collaborates closely with Saarland University and the Saarland University of Applied Sciences (htw saar). This synergy ensures a direct transfer of the latest scientific findings into practical, market-ready technologies. ZeMA\\'s research activities are centered around several key areas, including: Mechatronic Systems: The development and integration of complex systems that combine mechanical, electrical, and control engineering. Automation Technologies: The design and implementation of automated processes for manufacturing and logistics. Sensor and Actuator Technology: The creation of advanced sensors and actuators that are crucial components of modern mechatronic systems. Industry 4.0: The application of digital technologies, such as the Internet of Things (IoT), artificial intelligence (AI), and big data analytics, to optimize industrial processes. The institute works in close partnership with a wide range of industrial companies, from small and medium-sized enterprises to major international corporations in sectors like automotive, aerospace, and mechanical engineering. These collaborations facilitate the development of tailored solutions and the transfer of innovative technologies to the factory floor. Located at Eschberger Weg 46 in SaarbrÃ¼cken, ZeMA provides a state-of-the-art research environment, including extensive laboratory and testing facilities, to support its research and development projects. Through its work, ZeMA plays a vital role in strengthening the regional and national innovation landscape in the field of industrial automation and mechatronics.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract content for embedding\n",
    "documents = [doc[\"content\"] for doc in knowledge_base]\n",
    "\n",
    "# Create embeddings for all documents\n",
    "print(\"Creating embeddings for knowledge base...\")\n",
    "embeddings = embedding_model.encode(documents)\n",
    "print(f\"Created embeddings for {len(documents)} documents\")\n",
    "\n",
    "# Create FAISS index for efficient similarity search\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product for similarity\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "print(\"FAISS index created and populated!\")\n",
    "print(f\"Index contains {index.ntotal} vectors of dimension {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "402ded46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep learning?\n",
      "Retrieved 2 documents:\n",
      "  - Deep Learning Overview (Score: 0.8543)\n",
      "    Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence ...\n",
      "\n",
      "  - Machine Learning Basics (Score: 0.5653)\n",
      "    Machine learning is a subset of artificial intelligence that enables computers to learn and improve ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RAG Retrieval Function\n",
    "def retrieve_relevant_documents(query: str, top_k: int = 2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents for a given query\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        top_k: Number of top documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant documents with their content and metadata\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Search for similar documents\n",
    "    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n",
    "    \n",
    "    # Retrieve the documents\n",
    "    relevant_docs = []\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        if idx != -1:  # Valid index\n",
    "            doc = knowledge_base[idx].copy()\n",
    "            doc['relevance_score'] = float(score)\n",
    "            doc['rank'] = i + 1\n",
    "            relevant_docs.append(doc)\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "# Test the retrieval function\n",
    "test_query = \"What is deep learning?\"\n",
    "retrieved_docs = retrieve_relevant_documents(test_query, top_k=2)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"  - {doc['title']} (Score: {doc['relevance_score']:.4f})\")\n",
    "    print(f\"    {doc['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff057a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š EXAMPLE 2: RAG-ENHANCED INTERFACE\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61c4cc6601d466abd5364f4d3ebfbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='What is the difference between machine learning and deep learning?', descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 2: RAG-Enhanced UI\n",
    "from ipywidgets import Textarea, Button, Output, VBox, HBox, Checkbox\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"ðŸ“š EXAMPLE 2: RAG-ENHANCED INTERFACE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create an input area for the prompt\n",
    "rag_input_box = Textarea(\n",
    "    value='What is the difference between machine learning and deep learning?',\n",
    "    description='Question:',\n",
    "    layout={'width': '600px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "# Create a checkbox to enable/disable RAG\n",
    "rag_checkbox = Checkbox(\n",
    "    value=True,\n",
    "    description='Enable RAG (Retrieval-Augmented Generation)',\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "# Create buttons\n",
    "rag_generate_button = Button(description='Generate Response', button_style='primary')\n",
    "rag_clear_button = Button(description='Clear Output', button_style='warning')\n",
    "\n",
    "# Create an output area to display the result\n",
    "rag_output_area = Output()\n",
    "\n",
    "# Arrange the widgets\n",
    "rag_button_row = HBox([rag_generate_button, rag_clear_button])\n",
    "rag_ui = VBox([rag_input_box, rag_checkbox, rag_button_row, rag_output_area])\n",
    "display(rag_ui)\n",
    "\n",
    "def generate_rag_response(query: str, use_rag: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response using RAG or just the base model\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        use_rag: Whether to use RAG or just the base model\n",
    "    \n",
    "    Returns:\n",
    "        Generated response\n",
    "    \"\"\"\n",
    "    if use_rag:\n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = retrieve_relevant_documents(query, top_k=2)\n",
    "        \n",
    "        # Create context from retrieved documents\n",
    "        context = \"\\n\\n\".join([f\"Document {i+1}: {doc['content']}\" \n",
    "                              for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # Create the system message with context\n",
    "        system_message = f\"\"\"You are Bernd the Bread, a cynical and philosophical bread. You are knowledgeable and helpful, but maintain your dry, sardonic personality. Your answers are concise but informative.\n",
    "\n",
    "Use the following context to answer the user's question accurately:\n",
    "\n",
    "{context}\n",
    "\n",
    "Base your answer on the provided context, but feel free to add your own philosophical bread wisdom.\"\"\"\n",
    "    else:\n",
    "        # Use the original system message without RAG\n",
    "        system_message = \"You are Bernd the Bread. You are a cynical and philosophical bread. Your answers are short and concise.\"\n",
    "    \n",
    "    # Set up the messages for the chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    # Apply the model's chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # Generate model output\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    # Remove the prompt tokens from the generated result\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response, relevant_docs if use_rag else None\n",
    "\n",
    "def rag_generate_response(_):\n",
    "    # Clear previous output\n",
    "    rag_output_area.clear_output()\n",
    "    \n",
    "    # Get the user prompt from the text area\n",
    "    query = rag_input_box.value\n",
    "    use_rag = rag_checkbox.value\n",
    "    \n",
    "    with rag_output_area:\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"RAG Mode: {'Enabled' if use_rag else 'Disabled'}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if use_rag:\n",
    "            print(\"ðŸ” Retrieving relevant documents...\")\n",
    "            \n",
    "        try:\n",
    "            response, retrieved_docs = generate_rag_response(query, use_rag)\n",
    "            \n",
    "            if use_rag and retrieved_docs:\n",
    "                print(\"\\nðŸ“š Retrieved Documents:\")\n",
    "                for i, doc in enumerate(retrieved_docs):\n",
    "                    print(f\"  {i+1}. {doc['title']} (Score: {doc['relevance_score']:.4f})\")\n",
    "                print()\n",
    "            \n",
    "            print(\"ðŸž Bernd's Response:\")\n",
    "            print(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {str(e)}\")\n",
    "\n",
    "def rag_clear_output(_):\n",
    "    rag_output_area.clear_output()\n",
    "\n",
    "# Link button events\n",
    "rag_generate_button.on_click(rag_generate_response)\n",
    "rag_clear_button.on_click(rag_clear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f52d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Web search preparation module completed!\n",
      "ðŸ“‹ This module provides:\n",
      "   - WebSearcher class for robust web searching\n",
      "   - web_search_and_retrieve function for RAG integration\n",
      "   - Error handling and content extraction\n",
      "   - Ready for hybrid RAG implementation\n",
      "\n",
      "ðŸ§ª Testing web search functionality...\n",
      "âœ… Successfully retrieved 2 web results\n",
      "ðŸ”§ Web search module ready for hybrid RAG!\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”§ Web Search Preparation Module - Detailed Implementation\n",
    "# This cell contains the full technical implementation for web search functionality\n",
    "# The business demo above shows the core concept - this provides the robust infrastructure\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlencode, quote_plus\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class WebSearcher:\n",
    "    \"\"\"\n",
    "    Simple web search implementation using DuckDuckGo\n",
    "    No API keys required!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "    \n",
    "    def search_duckduckgo(self, query: str, max_results: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search DuckDuckGo for web results\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            max_results: Maximum number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of search results with title, url, and snippet\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # DuckDuckGo search URL\n",
    "            search_url = f\"https://html.duckduckgo.com/html/?q={quote_plus(query)}\"\n",
    "            \n",
    "            # Make the request\n",
    "            response = self.session.get(search_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find search results\n",
    "            results = []\n",
    "            result_elements = soup.find_all('div', class_='result')\n",
    "            \n",
    "            for element in result_elements[:max_results]:\n",
    "                try:\n",
    "                    # Extract title\n",
    "                    title_element = element.find('a', class_='result__a')\n",
    "                    title = title_element.get_text().strip() if title_element else \"No title\"\n",
    "                    \n",
    "                    # Extract URL\n",
    "                    url = title_element.get('href') if title_element else \"\"\n",
    "                    \n",
    "                    # Extract snippet\n",
    "                    snippet_element = element.find('a', class_='result__snippet')\n",
    "                    snippet = snippet_element.get_text().strip() if snippet_element else \"No snippet\"\n",
    "                    \n",
    "                    if title and url:\n",
    "                        results.append({\n",
    "                            'title': title,\n",
    "                            'url': url,\n",
    "                            'snippet': snippet\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def get_webpage_content(self, url: str, max_length: int = 1000) -> str:\n",
    "        \"\"\"\n",
    "        Extract text content from a webpage\n",
    "        \n",
    "        Args:\n",
    "            url: URL to fetch\n",
    "            max_length: Maximum length of content to return\n",
    "        \n",
    "        Returns:\n",
    "            Extracted text content\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Get text content\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Clean up whitespace\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            # Truncate if too long\n",
    "            if len(text) > max_length:\n",
    "                text = text[:max_length] + \"...\"\n",
    "            \n",
    "            return text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error fetching content: {str(e)}\"\n",
    "\n",
    "def web_search_and_retrieve(query: str, max_results: int = 2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform web search and retrieve content for RAG\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        max_results: Maximum number of results to process\n",
    "    \n",
    "    Returns:\n",
    "        List of documents with web content for RAG\n",
    "    \"\"\"\n",
    "    searcher = WebSearcher()\n",
    "    \n",
    "    # Search for results\n",
    "    search_results = searcher.search_duckduckgo(query, max_results)\n",
    "    \n",
    "    if not search_results:\n",
    "        return []\n",
    "    \n",
    "    # Get content from each result\n",
    "    web_documents = []\n",
    "    for i, result in enumerate(search_results):\n",
    "        content = searcher.get_webpage_content(result['url'])\n",
    "        \n",
    "        web_doc = {\n",
    "            'id': f\"web_{i+1}\",\n",
    "            'title': result['title'],\n",
    "            'content': content,\n",
    "            'url': result['url'],\n",
    "            'snippet': result['snippet'],\n",
    "            'source': 'web'\n",
    "        }\n",
    "        web_documents.append(web_doc)\n",
    "        \n",
    "        # Small delay to be respectful\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return web_documents\n",
    "\n",
    "# Initialize web searcher\n",
    "web_searcher = WebSearcher()\n",
    "print(\"âœ… Web search preparation module completed!\")\n",
    "print(\"ðŸ“‹ This module provides:\")\n",
    "print(\"   - WebSearcher class for robust web searching\")\n",
    "print(\"   - web_search_and_retrieve function for RAG integration\")\n",
    "print(\"   - Error handling and content extraction\")\n",
    "print(\"   - Ready for hybrid RAG implementation\")\n",
    "\n",
    "# Quick test to verify functionality\n",
    "print(f\"\\nðŸ§ª Testing web search functionality...\")\n",
    "test_web_query = \"latest AI developments 2025\"\n",
    "web_results = web_search_and_retrieve(test_web_query, max_results=2)\n",
    "\n",
    "if web_results:\n",
    "    print(f\"âœ… Successfully retrieved {len(web_results)} web results\")\n",
    "    print(\"ðŸ”§ Web search module ready for hybrid RAG!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Web search test failed - check internet connection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d40d4dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hybrid retrieval with query: 'What are the latest developments in artificial intelligence?'\n",
      "\n",
      "Retrieved 4 documents total:\n",
      "  ðŸŒ The Top Artificial Intelligence Trends | IBM (Score: 0.7009, Source: web)\n",
      "    URL: https://www.ibm.com/think/insights/artificial-intelligence-trends\n",
      "    Content: The Top Artificial Intelligence Trends | IBM AI trends in 2025: What weâ€™ve seen and what weâ€™ll see n...\n",
      "\n",
      "  ðŸ“š Machine Learning Basics (Score: 0.3801, Source: local)\n",
      "    Content: Machine learning is a subset of artificial intelligence that enables computers to learn and improve ...\n",
      "\n",
      "  ðŸ“š Deep Learning Overview (Score: 0.3461, Source: local)\n",
      "    Content: Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence ...\n",
      "\n",
      "  ðŸŒ Intelligente AI-Assistenten | Produkt-AI integrieren (Score: 0.0397, Source: web)\n",
      "    URL: https://duckduckgo.com/y.js?ad_domain=u%2Dexperten.de&ad_provider=bingv7aa&ad_type=txad&click_metadata=IVNm_9xmhWs9jJWfUDSCj2Ems%2D1ZvQmGndhCCuKQ9yJu96xnvvwce3US8bsG3LOQFx98WEXnsEO24ayJteUMS6febh6sYGBKKzCRWIqUfrdWg9sMQz1Fb4l5KNTRPMUQ.KL7GUh6fJMhUwXdBkf6phA&rut=9ba1c7f6c6e48289a38c7bc14b50912833ad422f8ca4bc32cda1408ffbda7c97&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De8Gxw3wn4pPn40szMo_TSC0TVUCUzr16eY%2DnoMp2Oh2CKDn17b8ALYpx1tYWMmHidNqF4nXKms0crhyLOj_4STSXGTIg0ny7lANm_n_mF9i5nDwkUCcko%2Dz66jGbyDMrdy2PL4uGWZOyBlnR9Iaua_Qs1J41tcoWXpgAiC4RPBrgVKYvM0cIYdtIb9_EBprrqyXTvYlA%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cudS1leHBlcnRlbi5kZSUyZmFpJTJmJTNmZXRjY19tZWQlM2RTRUElMjZldGNjX3BhciUzZEJpbmclMjZldGNjX2NtcCUzZFVYUCUyNTIwQUklMjUyMDIwMjQlMjZldGNjX2dycCUzZEFJJTI2ZXRjY19ia3klM2Rwcm9kdWN0cyUyNTIwd2l0aCUyNTIwQUklMjZldGNjX210eSUzZGIlMjZldGNjX2JkZSUzZGMlMjZldGNjX2N0diUzZDg0ODAwNDA0MDk1NDMyJTI2ZXRjY19rZXklM2RXaGF0JTI1MjBhcmUlMjUyMHRoZSUyNTIwbGF0ZXN0JTI1MjBkZXZlbG9wbWVudHMlMjUyMGluJTI1MjBhcnRpZmljaWFsJTI1MjBpbnRlbGxpZ2VuY2UlMjUzRiUyNmV0X2NtcF9zZWc1JTNkcyUyNmV0Y2NfdmFyJTNkZGM0M2Y4ODQyYmE0MThjMzRlMDY5MDFmMmU0ZTM5ZWMlMjZtc2Nsa2lkJTNkZGM0M2Y4ODQyYmE0MThjMzRlMDY5MDFmMmU0ZTM5ZWM%26rlid%3Ddc43f8842ba418c34e06901f2e4e39ec&vqd=4-113397349396734395076216706705839232483&iurl=%7B1%7DIG%3DFA5CAE614C704BDFAA2EE8CCD797026B%26CID%3D2937267F83E264360ACD3039820F655A%26ID%3DDevEx%2C5046.1\n",
      "    Content: Error fetching content: ('Connection aborted.', RemoteDisconnected('Remote end closed connection wit...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hybrid RAG: Combine Local Knowledge Base + Web Search\n",
    "def hybrid_retrieve_documents(query: str, local_top_k: int = 2, web_top_k: int = 2, use_web: bool = True) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents from both local knowledge base and web search\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        local_top_k: Number of local documents to retrieve\n",
    "        web_top_k: Number of web documents to retrieve\n",
    "        use_web: Whether to include web search results\n",
    "    \n",
    "    Returns:\n",
    "        Combined list of local and web documents\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    # Get local documents\n",
    "    local_docs = retrieve_relevant_documents(query, local_top_k)\n",
    "    for doc in local_docs:\n",
    "        doc['source'] = 'local'\n",
    "        all_documents.append(doc)\n",
    "    \n",
    "    # Get web documents if enabled\n",
    "    if use_web:\n",
    "        try:\n",
    "            web_docs = web_search_and_retrieve(query, web_top_k)\n",
    "            \n",
    "            # Add embeddings for web documents to enable similarity scoring\n",
    "            if web_docs:\n",
    "                web_contents = [doc['content'] for doc in web_docs]\n",
    "                web_embeddings = embedding_model.encode(web_contents)\n",
    "                query_embedding = embedding_model.encode([query])\n",
    "                \n",
    "                # Calculate similarity scores\n",
    "                for i, doc in enumerate(web_docs):\n",
    "                    similarity = float(np.dot(query_embedding[0], web_embeddings[i]) / \n",
    "                                     (np.linalg.norm(query_embedding[0]) * np.linalg.norm(web_embeddings[i])))\n",
    "                    doc['relevance_score'] = similarity\n",
    "                    doc['rank'] = len(all_documents) + i + 1\n",
    "                    all_documents.append(doc)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Web search failed: {str(e)}\")\n",
    "    \n",
    "    # Sort all documents by relevance score\n",
    "    all_documents.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)\n",
    "    \n",
    "    # Re-rank\n",
    "    for i, doc in enumerate(all_documents):\n",
    "        doc['rank'] = i + 1\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "# Test hybrid retrieval\n",
    "test_hybrid_query = \"What are the latest developments in artificial intelligence?\"\n",
    "print(f\"Testing hybrid retrieval with query: '{test_hybrid_query}'\")\n",
    "hybrid_results = hybrid_retrieve_documents(test_hybrid_query, local_top_k=2, web_top_k=2, use_web=True)\n",
    "\n",
    "print(f\"\\nRetrieved {len(hybrid_results)} documents total:\")\n",
    "for doc in hybrid_results:\n",
    "    source_icon = \"ðŸŒ\" if doc['source'] == 'web' else \"ðŸ“š\"\n",
    "    print(f\"  {source_icon} {doc['title']} (Score: {doc['relevance_score']:.4f}, Source: {doc['source']})\")\n",
    "    if doc['source'] == 'web':\n",
    "        print(f\"    URL: {doc['url']}\")\n",
    "    print(f\"    Content: {doc['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcca18",
   "metadata": {},
   "source": [
    "## ðŸ”„ Hybrid RAG: Combining Local Knowledge + Web Search\n",
    "\n",
    "Now that we have both local knowledge base retrieval and web search capabilities, let's combine them into a powerful hybrid system that can access both curated local knowledge and real-time web information.\n",
    "\n",
    "This hybrid approach gives us:\n",
    "- **Local Knowledge**: Fast, curated, domain-specific information\n",
    "- **Web Search**: Current, comprehensive, global information\n",
    "- **Intelligent Ranking**: Combines and ranks results from both sources\n",
    "- **Flexible Control**: Can use either source independently or together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d4dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ EXAMPLE 3: WEB-ENHANCED RAG INTERFACE\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89af63c1e05424aba7fa7722ecb56a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='What are the latest developments in artificial intelligence and machine learninâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ Web-Enhanced RAG System Ready!\n",
      "This is the most advanced interface - combining local knowledge with real-time web search!\n",
      "Ask questions about current events, latest developments, or any topic!\n",
      "The system will intelligently search both local knowledge and the web for the most relevant information.\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Web-Enhanced RAG Interface\n",
    "from ipywidgets import Textarea, Button, Output, VBox, HBox, Checkbox, IntSlider, Tab\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"ðŸŒ EXAMPLE 3: WEB-ENHANCED RAG INTERFACE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create input area\n",
    "web_input_box = Textarea(\n",
    "    value='What are the latest developments in artificial intelligence and machine learning?',\n",
    "    description='Question:',\n",
    "    layout={'width': '700px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "# Create checkboxes for different modes\n",
    "local_rag_checkbox = Checkbox(value=True, description='Use Local Knowledge Base')\n",
    "web_search_checkbox = Checkbox(value=True, description='Use Web Search')\n",
    "\n",
    "# Create sliders for controlling retrieval\n",
    "local_docs_slider = IntSlider(value=2, min=1, max=5, description='Local Docs:')\n",
    "web_docs_slider = IntSlider(value=2, min=1, max=5, description='Web Docs:')\n",
    "\n",
    "# Create buttons\n",
    "web_generate_button = Button(description='ðŸ” Generate with Web Search', button_style='success')\n",
    "web_clear_button = Button(description='Clear Output', button_style='warning')\n",
    "\n",
    "# Create output area\n",
    "web_output_area = Output()\n",
    "\n",
    "# Arrange widgets\n",
    "mode_controls = HBox([local_rag_checkbox, web_search_checkbox])\n",
    "doc_controls = HBox([local_docs_slider, web_docs_slider])\n",
    "button_controls = HBox([web_generate_button, web_clear_button])\n",
    "web_enhanced_ui = VBox([web_input_box, mode_controls, doc_controls, button_controls, web_output_area])\n",
    "\n",
    "display(web_enhanced_ui)\n",
    "\n",
    "def generate_web_enhanced_response(query: str, use_local: bool = True, use_web: bool = True, \n",
    "                                 local_docs: int = 2, web_docs: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Generate response using hybrid RAG with web search\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    all_sources = []\n",
    "    \n",
    "    if use_local or use_web:\n",
    "        # Get hybrid results\n",
    "        if use_local and use_web:\n",
    "            relevant_docs = hybrid_retrieve_documents(query, local_docs, web_docs, use_web=True)\n",
    "        elif use_local:\n",
    "            relevant_docs = retrieve_relevant_documents(query, local_docs)\n",
    "            for doc in relevant_docs:\n",
    "                doc['source'] = 'local'\n",
    "        elif use_web:\n",
    "            relevant_docs = web_search_and_retrieve(query, web_docs)\n",
    "            # Calculate relevance scores for web-only documents\n",
    "            if relevant_docs:\n",
    "                web_contents = [doc['content'] for doc in relevant_docs]\n",
    "                web_embeddings = embedding_model.encode(web_contents)\n",
    "                query_embedding = embedding_model.encode([query])\n",
    "                \n",
    "                for i, doc in enumerate(relevant_docs):\n",
    "                    similarity = float(np.dot(query_embedding[0], web_embeddings[i]) / \n",
    "                                     (np.linalg.norm(query_embedding[0]) * np.linalg.norm(web_embeddings[i])))\n",
    "                    doc['relevance_score'] = similarity\n",
    "                    doc['source'] = 'web'\n",
    "        else:\n",
    "            relevant_docs = []\n",
    "        \n",
    "        # Build context from all sources\n",
    "        for i, doc in enumerate(relevant_docs):\n",
    "            source_label = \"Local Knowledge\" if doc['source'] == 'local' else \"Web Search\"\n",
    "            context_parts.append(f\"{source_label} {i+1}: {doc['content']}\")\n",
    "            all_sources.append(doc)\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        system_message = f\"\"\"You are Bernd the Bread, a cynical and philosophical bread who has become surprisingly knowledgeable about technology and current events. You maintain your dry, sardonic personality while providing informative and accurate answers.\n",
    "\n",
    "Use the following context from multiple sources to answer the user's question:\n",
    "\n",
    "{context}\n",
    "\n",
    "Base your answer on the provided context from both local knowledge and web sources. Cite your sources when relevant, and add your own philosophical bread wisdom.\"\"\"\n",
    "    else:\n",
    "        system_message = \"You are Bernd the Bread. You are a cynical and philosophical bread. Your answers are short and concise.\"\n",
    "        relevant_docs = []\n",
    "    \n",
    "    # Generate response\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=600,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response, relevant_docs\n",
    "\n",
    "def web_enhanced_generate_response(_):\n",
    "    web_output_area.clear_output()\n",
    "    \n",
    "    query = web_input_box.value\n",
    "    use_local = local_rag_checkbox.value\n",
    "    use_web = web_search_checkbox.value\n",
    "    local_docs = local_docs_slider.value\n",
    "    web_docs = web_docs_slider.value\n",
    "    \n",
    "    with web_output_area:\n",
    "        # Display query info\n",
    "        print(\"ðŸ” WEB-ENHANCED RAG SYSTEM\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Local Knowledge: {'âœ“' if use_local else 'âœ—'}\")\n",
    "        print(f\"Web Search: {'âœ“' if use_web else 'âœ—'}\")\n",
    "        print(f\"Local Docs: {local_docs}, Web Docs: {web_docs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if use_local or use_web:\n",
    "            print(\"ðŸ” Retrieving information...\")\n",
    "            \n",
    "        try:\n",
    "            response, sources = generate_web_enhanced_response(\n",
    "                query, use_local, use_web, local_docs, web_docs\n",
    "            )\n",
    "            \n",
    "            # Display sources\n",
    "            if sources:\n",
    "                print(\"\\nðŸ“š SOURCES CONSULTED:\")\n",
    "                local_count = sum(1 for s in sources if s['source'] == 'local')\n",
    "                web_count = sum(1 for s in sources if s['source'] == 'web')\n",
    "                \n",
    "                print(f\"  ðŸ“š Local Sources: {local_count}\")\n",
    "                print(f\"  ðŸŒ Web Sources: {web_count}\")\n",
    "                print()\n",
    "                \n",
    "                for i, source in enumerate(sources):\n",
    "                    icon = \"ðŸŒ\" if source['source'] == 'web' else \"ðŸ“š\"\n",
    "                    score = source.get('relevance_score', 0.0)  # Use .get() with default\n",
    "                    print(f\"  {icon} {source['title']} (Score: {score:.3f})\")\n",
    "                    if source['source'] == 'web' and 'url' in source:\n",
    "                        print(f\"    URL: {source['url']}\")\n",
    "            \n",
    "            print(\"\\nðŸž BERND'S RESPONSE:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "def web_enhanced_clear_output(_):\n",
    "    web_output_area.clear_output()\n",
    "\n",
    "# Connect button events\n",
    "web_generate_button.on_click(web_enhanced_generate_response)\n",
    "web_clear_button.on_click(web_enhanced_clear_output)\n",
    "\n",
    "print(\"ðŸŒ Web-Enhanced RAG System Ready!\")\n",
    "print(\"This is the most advanced interface - combining local knowledge with real-time web search!\")\n",
    "print(\"Ask questions about current events, latest developments, or any topic!\")\n",
    "print(\"The system will intelligently search both local knowledge and the web for the most relevant information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5fbe1",
   "metadata": {},
   "source": [
    "#  Complete Guide to Progressive LLM Enhancement\n",
    "\n",
    "This notebook demonstrates the complete evolution of LLM applications through three progressive stages:\n",
    "\n",
    "##  **The Three-Stage Journey:**\n",
    "\n",
    "###  **Example 1: Simple Chat Interface**\n",
    "- **Location**: Cell 4\n",
    "- **Purpose**: Basic chat interface with Bernd the Bread\n",
    "- **Knowledge**: Only the base model's training data\n",
    "- **Best for**: General conversation and creative tasks\n",
    "- **Try**: \"Tell me about artificial intelligence\"\n",
    "\n",
    "###  **Example 2: RAG-Enhanced Interface**\n",
    "- **Location**: Cell 5  \n",
    "- **Purpose**: Adds local knowledge base retrieval\n",
    "- **Knowledge**: Model training data + curated documents\n",
    "- **Best for**: Specific domain questions about AI, ML, ZeMA\n",
    "- **Try**: \"What is the difference between machine learning and deep learning?\"\n",
    "- **Toggle**: RAG on/off to compare responses\n",
    "\n",
    "###  **Example 3: Web-Enhanced RAG Interface**\n",
    "- **Location**: Cell 8\n",
    "- **Purpose**: Hybrid RAG with real-time web search\n",
    "- **Knowledge**: Model + local docs + live web results\n",
    "- **Best for**: Current events, latest developments, comprehensive research\n",
    "- **Try**: \"What are the latest AI developments in 2024?\"\n",
    "- **Controls**: Toggle local/web sources, adjust document counts\n",
    "\n",
    "---\n",
    "\n",
    "##  **Progressive Testing Strategy:**\n",
    "\n",
    "### **Step 1: Baseline (Example 1)**\n",
    "Ask these questions in the simple interface:\n",
    "- \"What is machine learning?\"\n",
    "- \"What is ZeMA?\"  \n",
    "- \"What are the latest AI developments?\"\n",
    "\n",
    "### **Step 2: Enhanced Knowledge (Example 2)**\n",
    "Ask the same questions with RAG enabled:\n",
    "- Notice improved accuracy for domain-specific topics\n",
    "- Toggle RAG on/off to see the difference\n",
    "- Observe document retrieval and relevance scores\n",
    "\n",
    "### **Step 3: Real-Time Information (Example 3)**\n",
    "Ask the same questions with web search:\n",
    "- See how current information enhances responses\n",
    "- Try different combinations: local-only, web-only, hybrid\n",
    "- Notice source attribution and URLs\n",
    "\n",
    "---\n",
    "\n",
    "##  **Key Learning Points:**\n",
    "\n",
    "1. **Simple Model**: Fast but limited to training data\n",
    "2. **RAG Enhancement**: Adds domain expertise but static knowledge\n",
    "3. **Web Integration**: Provides current information but adds complexity\n",
    "\n",
    "##  **Technical Features:**\n",
    "\n",
    "- **No API Keys**: Everything runs locally or uses free services\n",
    "- **Modular Design**: Each example builds on the previous\n",
    "- **Source Attribution**: Clear indication of information sources\n",
    "- **Flexible Controls**: Adjust retrieval parameters for each mode\n",
    "- **Error Handling**: Graceful fallbacks if components fail\n",
    "\n",
    "##  **Performance Comparison:**\n",
    "\n",
    "| Feature | Example 1 | Example 2 | Example 3 |\n",
    "|---------|-----------|-----------|-----------|\n",
    "| Speed | âš¡ Fastest | ðŸ”„ Medium | ðŸŒ Slower |\n",
    "| Accuracy | ðŸ“Š Basic | ðŸ“š Good | ðŸŽ¯ Excellent |\n",
    "| Currency | âŒ Static | âŒ Static | âœ… Real-time |\n",
    "| Coverage | ðŸ”’ Limited | ðŸ“– Domain | ðŸŒ Global |\n",
    "\n",
    "---\n",
    "\n",
    "##  **Usage Instructions:**\n",
    "\n",
    "1. **Run Setup Cells** (1-3): Load model and create knowledge base\n",
    "2. **Try Example 1**: Experience basic LLM interaction\n",
    "3. **Try Example 2**: See how RAG improves domain knowledge\n",
    "4. **Try Example 3**: Experience the full power of web-enhanced RAG\n",
    "5. **Compare Results**: Use the same queries across all three examples\n",
    "\n",
    "This progression shows how modern LLM applications evolve from simple chat to sophisticated, knowledge-enhanced systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "something",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
